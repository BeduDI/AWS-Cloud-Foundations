# ¿Dónde comienza todo?

No se puede concebir el crecimiento en los negocios sin el advenimiento de las tecnolgías en la nube.
Tradicionalmente tener un servidor en tu propio centro de datos no solo requería la infraestructura necesaria (hablamos de electricidad, control de temperatura y humedad, energía de respaldo, y un largo etc), el recurso humano altamente capacitado en ámbitos como redes y servidores hacían practicamente inalcansable para organizaciones medianas o pequeñas poder contar software corriendo en servidores que impulsara sus negocios o procesos educativos por poner un ejemplo.

Aunque suene extraño, el concepto de compartir tiempo y recursos de un sistema de cómputo no es algo que se haya dado en las últimas décadas, ni siquiera en este siglo, [John McCarthy](https://es.wikipedia.org/wiki/John_McCarthy) a finales de la década de los 50 ya había planteado e implementado un sistema donde múltiples personas podían compartir CPU, memoria y tiempo de ejecución en una IBM 704. El concepto se hizo muy popular en décadas siguientes, pero ya para la década de los 80 comienza a perder polularidad debido al auge de los microprocesadores cada vez mas rápidos y potentes.

El lanzamiento de la WWW con el protocolo de transferencia de hipertexto el 30 de Diciembre del 1990 por [Tim Berners-Lee](https://en.wikipedia.org/wiki/Tim_Berners-Lee) marca el hito en la historia que haría que la demanda de sistemas de cómputo creciera en las décadas venideras, ya que la web se idealiza como algo abierto, libre para todos, editable y libre de cargos o patentes en teoría cualquier persona podía hacer uso de esta nueva tecnología para compartir sus ideas al mundo entero, el problema, servidores. 
Para hacer funcionar una web en ese momento se requerían un `web server`, siendo httpd el primero escrito por el propio Tim (hoy día también además de otras tecnologías), no era muy pesado para ser ejecutado pero hay que recordar el contexto temporal en que los recursos de cómputo eran practicamente limitaods a empresas con gran capital o instituciones educativas y de investigación, así que solo unos privilegiados podían generar sus propias páginas y compartirlas al mundo. No fué hasta 1994 cuando se lanzan los primeros servidores para ser montdos en un rack por parte de Compaq, los [ProLian](https://iweb.com/wp-content/uploads/2018/09/5.jpg), ese fué el primero paso hacia centros llenos de computadoras potentes para el momento. 

Uno de los precursores de renta de computadoras con conexion a internet para tener tu propia página web corriendo fué Richard Yoo, él en 1996 ya servía como un ISP pequeño y para 1997 ofrecía servicio de web hosting, despues de algunos negocios en 1998 se funda `RackSpace` con Richard Yoo como CEO, con la filosofía de la compañía ademas de ofrecer servicios como web hosting se enfocaba a servir soporte de nivel "fanático", es decir un servicio con la primica en el soporte y servicio al cliente.  Recordemos que a este punto la burbuja de [las punto com](https://blog.r4.com/burbuja-de-las-puntocom/) estaba en plena gestión, todo mundo quería una web no importando en realidad si era 
negocio en el corto plazo, el punto era hacer una página y luego en el futuro monetizarla.

Como ellos muchos otros nacieron en el mercado con la promesa de brindarte un espacio donde podias alojar primero tus páginas web, pero después con la explosion del lenguaje PHP y mas tarde de CMS como Wordpress los hosting ya ofrecían tambien soporte para este lenguaje y base de datos, todo en uno, una real ganga. Esto le dió otro empojón al la demanda centrada en productos de software de tipo web, pero la alta demanda de algunos sitios en especial de noticias comienza a demandas arquitecturas y recursos mas allá de los que podía brindar un hosting de un solo servidor, por lo regular eran servidores que no podías modificar mucho, la base de datos ya venía definida, el lenguaje para trabajar también, algunos ofrecen servicio de email integrado que también ya viene definido, otro problema que se sucitaba también es el ¿en quien confiar?, un día tu web hosting podría desaparecer sin dejar rastro, en cuanto a los costos, se cobra normalmente una tarifa plana con ciertos recursos que no puedes pasar del mes, si los pasa te es cobrado un cargo extra, si los tienes sub utilizados no puedes quitarte el costo extra, ante picos de operación son poco flexibles.

La evolución de la web a la llamada [Web 2.0](https://whatis.techtarget.com/definition/Web-20-or-Web-2) aceleró indudablemente la necesidad por más servicios, mas rápidos y mas flexibles, la Web 2.0 se caracteriza por si dinamismo de las nuevas generaciones de páginas, la interactividad que brindan, la capacidad de ver contenido mas rico visualmente son altamente atractivos por lo que la demanda de paginas web mas complejas sube. Es en esta etapa donde podemos hablar de cómputo móvil, software como servicio, web apps, generación de contenido en audio y video, comunicaciones unificadas y el ya conocido social media.

El 14 de marzo de 2006 el tablero de juego iba a cambiar, Amazon lanza su servicio de cómputo en la nube, Aamazon Web Services, lanzando aparte de S3 (servicio de storage) y SQS (Servicio de colas) el servicio `Elastic Compute Cloud` mejor conocido como EC2, la peculiaridad y novedoso del servicio es el cobro de tarifas por la cantidad de recursos de uses, tanto en CPU y memoria RAM como en storage, se basa también en la modalida de 'auto servicio' con lo cual uno mismo puede levantar tantos servidores como sea necesario y configurarlos con el lenguaje de programación de la preferencia del cliente. Estos puntos de flexibilidad dieron a empresas pequeñas y medianas la capacidad de ser mas competitivas sin incurrir en los costos excesivos y el personal que requiere un servidor en una oficina. Aunque, es cierto, para operar AWS también requieres personal capacitado, con experiencia, que sepa y reconozca que servicios deben ser usados ante la necesidad de los diferentes negocios que requieren una solución tecnológica a su operación. Aquí es donde entras tú, capacitarte para las tecnologías de nube es una inversión que seguro recuperarás rápidamente.



# ¿Opciones comunes de cómputo?

Ya hemos hablado brevemente de las ventajas de tener nuestros servidores en algún proveedor de servicios en la nube, pero, ¿es la mejor opción?, bueno, hablemos mas al respecto.

Hoy día en el mercado podemos encontrar diversas ofertas de cómputo, por un lado podemos encontar los servidores instalados en sitio. Este tipo de servicio de cómputo se requiere a la hora de cumplir con normativas, ej: en industrias como las de seguros no es posible alojar los datos de los clientes fuera de México, por lo que un servicio de cómputo en la nube no es posible para la mayoría de servicios. Tener servicios en sitio implica un gran esfuerzo para mantenerlos operando constantemente, los servidores practicamente son computadoras con muchos gigas de memoria RAM, múltiples procesadores con múltiples núcleos cada uno, míltiples discos duros formado arreglos [RAID](https://searchstorage.techtarget.com/definition/RAID) que dependiendo de la configuración brindan respaldo a los datos, brindan velocidad o ambos, todo esto hace que los servidores desprendan mucho calor el cual al tener varios servidores apilados en [racks](https://www.capitolinetraining.com/getting-your-data-centre-ready-for-open-compute-and-open19-rack-layouts/) hacen que el lugar físico donde se encuentran tranquilamente pueda llegar a temperaturas de 70 grados, la temperatura es un enemigo a vencer siempre que usamos servidores en sitio, para lo cual se debe contar con un asolucion confiable de control de temperatura con equipos especiales de enfriamiento que deben estar encendidos por meses continuamente y que ademas garanticen parametros de humedad en el aire muy específicos, demasiada humedad causará condensación en nuestros servidores, poca humedad causará un aire en el centro de datos que es propenso a tener carga estática ¿te imaginas la memoria RAM de un procesador dañada por la descarga de cargas estáticas acumuladas?, para cerrar el tema del enfriamiento tambien se debe contar con una instalación fisica adecuada que garantice el correcto flujo tande de aire caliente despedido por los servidores comod e aire frio que debe entrar a ellos, [aquí](https://www.cisco.com/c/en/us/solutions/collateral/data-center-virtualization/unified-computing/white_paper_c11-680202.html) se puede profundizar mas al respecto. 

El siguiente enemigo a vencer es la electricidad, los servidores normalmente deben estar encendidos 24/7 por años inclusive por lo que se debe garantizar un flujo continuo de electricidad si llega a fallar el suministro público de electricidad, a lo cual se usan tradicionalmente equipos [UPS](https://www.energystar.gov/products/data_center_equipment/uninterruptible_power_supplies) que mantienen nuestros servidores encendidos por algunos minutos en lo que una [planta de energía](https://www.generatorsource.com/Supplying_Backup_Power_to_Data_Centers.aspx) arranca y regulariza el voltaje necesario para que los servidores operen, tanto como el o los UPS y la o las plantas de energía requieren de mantenimiento continuo a fin de evitar anomalías ante un corte impredecible de electricidad de la red principal.

La comunicacion entre [servidores](https://www.sdxcentral.com/data-center/definitions/data-center-networking-explained/) es crucial para tener servicios operando, sin problema tienes qu etener las grabaciones de tu servidor PBX habia un servidor de storage para recolectar las grabaciones de las llamadas por ejemplo. Para mantener una sana comunicación se requieren equipos de alta calidad, un buen cablead instalado por profesionales y una impecable administración y configucación de las redes, eso sin contar con la seguridad que conlleva con la correcta segmentacipn y dimensionamiento de las mismas,ah, la redundancia es también un factor importante. Algunos de los esquipso usados en la industria se presentan [aquí](https://www.router-switch.com/Price-cisco-switches-cisco-switch-catalyst-6500_c18)

Hasta aqui se presentan los principales problemas para arrancar un centro de datos, eventualmente los equipos fallan, se deterioran, hay que estar dando constante mantenimiento tanto al espacio físico, al propio hardware de los servidores tambien hay que hacerlos, los discos duros fallan y apesar de en arreglo RAID eso no exime de reemplazarlos si se presenta una falla en alguno de ellos, en cuanto a los sistemas operativos hay que estar aplicando parches de seguridad y coordinar ventanas de mantenimiento necesarias para mantener la usabilidad y seguridad.
Aqui entran en juego temas de [soporte y garantías](https://marketing.dell.com/Global/FileLib/hp_microsite/dell-support_services.pdf), si se daña un disco quien que va a dar el soporte y garantía para reemplazarlo, y ¿si un ventilador de tu servidor deja de funcionar?, ¿si alguna de las fuentes redundantes de un servidor deja de funcionar?, ¿si un puerto en una tarjeta de red deja de hacerlo?, todos escenarios plausibles que sudedan.

## Sub utilización y virtualización.
La complejidad en los centros de datos aunque solo cuenten con un rack en él (a un rack le puden caber 24 servidores físicos, 48 Unidades de rack, cada servidor puede ocupar 2 Unidades dependiendo del modelo) se vuelve compleja rápidamente, cada servicio como telefonía o aplicación web requiere tener servidores para manejar storage, servidores web, sftp, telefonía, bases de datos, dode dependiendo de la criticidad que se le asigne a cada servicio será determinante pasa saber en que servidor físico se configurarán los servicios. Imagina por un momento a un proveedor de hosting, aloja cientos de páginas y aplicaciones de sus clientes, realmente se vuelve inviabel económicamente tener un servidor para cada cliente a un precio accesible, lo mismo pasa en empresas y centros de investigación, no se vuelve viable tener servidores físicos para cada servicio. El problema de brindar varios servicios con un mismo servidor físico se da desde que hay servidores, múltiples personas deseaban utilizarlos pero no era posible, se opraron por estrategias de tiempo compartido al inicio donde el servidor podía procesar los datos de una persona mientras otra tomaba su tiempo para decidir cual sería el siguiente set de instrucciones que el servidor debía cumplir por medio de `terminales tontas`, lo cual era ineficiente, no era raro que un usuario sobrecargara al servidor afectando en rendimiento a los demas usuarios conectados afectando sus tareas. Fué hasta 1988 con el lanzamiento de [SoftPC](https://www.nytimes.com/1988/06/19/business/the-executive-computer-choosing-a-link-from-mac-to-dos.html) que la idea de la virtualizaión cobró vida.
La virtualización es en concepto, tomar un servidor físico y poder montar sobre de él mútiples sistemas operativos que incluso pueden ser diferentes entre sí, brindando aislamiento y seguridad, así los procesos de un sistema operativo se mantienen aislados de otros sistemas operativos, puedes tener así multiples servicios ejecutandose en su propio sistema operativo, ¿tienes 4 aplicacioes web con sus respectivas bases de datos?, no hay problema, puedes ejecutar cada tupla web server y base de datos en un sistema operativo en el mismo servidor físico. 